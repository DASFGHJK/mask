{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from lxml import etree\n",
    "import warnings\n",
    "\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "warnings.filterwarnings(\"ignore\", category=InsecureRequestWarning)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HenanDailyScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "            \"Accept-Language\": \"zh-CN,zh;q=0.9\",\n",
    "            \"Cache-Control\": \"no-cache\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Pragma\": \"no-cache\",\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "        }\n",
    "        self.month_days = {\n",
    "            1: 31, 2: 29, 3: 31, 4: 30,\n",
    "            5: 31, 6: 30, 7: 31, 8: 31,\n",
    "            9: 30, 10: 31, 11: 30, 12: 31\n",
    "        }\n",
    "        self.filename = '河南日报2024.csv'\n",
    "        self.base_url = \"https://newpaper.dahe.cn/hnrb/html\"\n",
    "\n",
    "        # 配置请求会话\n",
    "        self.session = requests.Session()\n",
    "        self.session.verify = False\n",
    "        self.session.headers.update(self.headers)\n",
    "\n",
    "    def run(self):\n",
    "        with open(self.filename, 'w', encoding='utf-8-sig', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['日期', '文章链接', '标题', '正文'])\n",
    "            self._process_all_dates(writer)\n",
    "\n",
    "    def _process_all_dates(self, writer):\n",
    "        for month in range(1, 13):\n",
    "            for day in range(1, self.month_days[month] + 1):\n",
    "                self._process_daily_data(month, day, writer)\n",
    "                time.sleep(1)  # 请求间隔\n",
    "\n",
    "    def _process_daily_data(self, month, day, writer):\n",
    "        date_str = f\"2024-{month:02d}-{day:02d}\"\n",
    "        list_url = f\"{self.base_url}/2024-{month:02d}/{day:02d}/node_1.htm?v=1\"\n",
    "\n",
    "        try:\n",
    "            list_response = self._safe_request(list_url)\n",
    "            if list_response and list_response.status_code == 200:\n",
    "                self._parse_list_page(list_response.text, month, day, date_str, writer)\n",
    "        except Exception as e:\n",
    "            print(f\"处理 {date_str} 数据时出错: {str(e)}\")\n",
    "\n",
    "    def _parse_list_page(self, html, month, day, date_str, writer):\n",
    "        tree = etree.HTML(html)\n",
    "        titles = tree.xpath(\"//ul[@class='news-list']/li[@class='news-item']/a/text()\")\n",
    "        relative_urls = tree.xpath(\"//li[@class='news-item']/a/@href\")\n",
    "\n",
    "        base_url = f\"{self.base_url}/2024-{month:02d}/{day:02d}/\"\n",
    "        print(f'正在抓取{month}月{day}日数据')\n",
    "        for title, rel_url in zip(titles, relative_urls):\n",
    "            article_url = base_url + rel_url\n",
    "            article_content = self._get_article_content(article_url)\n",
    "            self._write_row(writer, date_str, article_url, title.strip(), article_content)\n",
    "\n",
    "    def _get_article_content(self, url):\n",
    "        try:\n",
    "            response = self._safe_request(url)\n",
    "            if response and response.status_code == 200:\n",
    "                return self._parse_article_content(response.text)\n",
    "        except Exception as e:\n",
    "            print(f\"获取文章内容失败: {url} - {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "    def _parse_article_content(self, html):\n",
    "        tree = etree.HTML(html)\n",
    "        content_elements = tree.xpath(\"//div[@id='articleContent']//text()\")\n",
    "        cleaned = [re.sub(r'\\u3000', '', item.strip()) for item in content_elements if item.strip()]\n",
    "        return ' '.join(cleaned)\n",
    "\n",
    "    def _safe_request(self, url):\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            return self.session.get(url, timeout=10)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"请求失败: {url} - {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _write_row(self, writer, date, url, title, content):\n",
    "        writer.writerow([date, url, title, content])\n",
    "        print(f\"已写入: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m      2\u001B[0m     scraper \u001B[38;5;241m=\u001B[39m HenanDailyScraper()\n\u001B[0;32m----> 3\u001B[0m     \u001B[43mscraper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[24], line 28\u001B[0m, in \u001B[0;36mHenanDailyScraper.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     26\u001B[0m writer \u001B[38;5;241m=\u001B[39m csv\u001B[38;5;241m.\u001B[39mwriter(file)\n\u001B[1;32m     27\u001B[0m writer\u001B[38;5;241m.\u001B[39mwriterow([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m日期\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m文章链接\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m标题\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m正文\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 28\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_all_dates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwriter\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[24], line 34\u001B[0m, in \u001B[0;36mHenanDailyScraper._process_all_dates\u001B[0;34m(self, writer)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m day \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmonth_days[month] \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_daily_data(month, day, writer)\n\u001B[0;32m---> 34\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    scraper = HenanDailyScraper()\n",
    "    scraper.run()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
